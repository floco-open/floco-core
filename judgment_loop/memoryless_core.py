# memoryless_core.py

import json
import os
import anthropic
import openai
import requests
from typing import Optional

def load_loop_config(path="judgment_loop/loop_config.json"):
    with open(path, 'r', encoding='utf-8') as f:
        return json.load(f)

def is_triggered(user_input, config):
    for keyword in config["trigger_keywords"]:
        if keyword in user_input:
            return True
    return False

def is_transfer_triggered(user_input, config):
    """ì´ê´€ í‚¤ì›Œë“œ ê°ì§€"""
    if "transfer_keywords" not in config:
        return False
    for keyword in config["transfer_keywords"]:
        if keyword in user_input:
            return True
    return False

def is_model_switch_triggered(user_input, config):
    """ëª¨ë¸ ë³€ê²½ í‚¤ì›Œë“œ ê°ì§€"""
    if not config.get("model_switching", {}).get("enabled", False):
        return False, None
    
    switch_keywords = config.get("model_switching", {}).get("switch_keywords", [])
    available_models = config.get("model_switching", {}).get("available_models", [])
    
    for keyword in switch_keywords:
        if keyword in user_input:
            # í‚¤ì›Œë“œ ë’¤ì˜ ëª¨ë¸ëª… ì¶”ì¶œ
            parts = user_input.split(keyword)
            if len(parts) > 1:
                target_model = parts[1].strip()
                if target_model in available_models:
                    return True, target_model
    return False, None

def switch_model(config, target_model):
    """ëª¨ë¸ ë³€ê²½ (ë©”ëª¨ë¦¬ì—ì„œë§Œ)"""
    if target_model in config.get("model_options", {}):
        new_model_config = config["model_options"][target_model]
        config["model"] = new_model_config
        return True
    return False

def call_claude(prompt: str) -> str:
    """Anthropic Claude API í˜¸ì¶œ"""
    try:
        client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        response = client.messages.create(
            model="claude-3-sonnet-20240229",
            max_tokens=300,
            temperature=0.7,
            system="ë‹¹ì‹ ì€ FLOCO íŒë‹¨ ë£¨í”„ì˜ íŒë‹¨ ìƒì„±ê¸°ì…ë‹ˆë‹¤.",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.content[0].text
    except Exception as e:
        return f"[ERROR] Claude í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"

def call_openai(prompt: str, model: str = "gpt-4") -> str:
    """OpenAI GPT API í˜¸ì¶œ"""
    try:
        client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ FLOCO íŒë‹¨ ë£¨í”„ì˜ íŒë‹¨ ìƒì„±ê¸°ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=300,
            temperature=0.7
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"[ERROR] OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"

def call_gemini(prompt: str) -> str:
    """Google Gemini API í˜¸ì¶œ"""
    try:
        import google.generativeai as genai
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        model = genai.GenerativeModel('gemini-pro')
        response = model.generate_content(
            f"ë‹¹ì‹ ì€ FLOCO íŒë‹¨ ë£¨í”„ì˜ íŒë‹¨ ìƒì„±ê¸°ì…ë‹ˆë‹¤.\n\n{prompt}"
        )
        return response.text
    except Exception as e:
        return f"[ERROR] Gemini í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"

def call_ollama(prompt: str, model: str = "llama2") -> str:
    """Ollama ë¡œì»¬ ëª¨ë¸ í˜¸ì¶œ"""
    try:
        url = os.getenv("OLLAMA_URL", "http://localhost:11434/api/generate")
        data = {
            "model": model,
            "prompt": f"ë‹¹ì‹ ì€ FLOCO íŒë‹¨ ë£¨í”„ì˜ íŒë‹¨ ìƒì„±ê¸°ì…ë‹ˆë‹¤.\n\n{prompt}",
            "stream": False
        }
        response = requests.post(url, json=data)
        return response.json()["response"]
    except Exception as e:
        return f"[ERROR] Ollama í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"

def call_cohere(prompt: str) -> str:
    """Cohere API í˜¸ì¶œ"""
    try:
        import cohere
        co = cohere.Client(os.getenv("COHERE_API_KEY"))
        response = co.generate(
            model='command',
            prompt=f"ë‹¹ì‹ ì€ FLOCO íŒë‹¨ ë£¨í”„ì˜ íŒë‹¨ ìƒì„±ê¸°ì…ë‹ˆë‹¤.\n\n{prompt}",
            max_tokens=300,
            temperature=0.7
        )
        return response.generations[0].text
    except Exception as e:
        return f"[ERROR] Cohere í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"

def call_huggingface(prompt: str, model: str = "microsoft/DialoGPT-medium") -> str:
    """HuggingFace API í˜¸ì¶œ"""
    try:
        from transformers import pipeline
        generator = pipeline('text-generation', model=model)
        response = generator(
            f"ë‹¹ì‹ ì€ FLOCO íŒë‹¨ ë£¨í”„ì˜ íŒë‹¨ ìƒì„±ê¸°ì…ë‹ˆë‹¤.\n\n{prompt}",
            max_length=300,
            num_return_sequences=1
        )
        return response[0]['generated_text']
    except Exception as e:
        return f"[ERROR] HuggingFace í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"

def call_ai_model(prompt: str, model_config: dict) -> str:
    """í†µí•© AI ëª¨ë¸ í˜¸ì¶œ í•¨ìˆ˜"""
    provider = model_config.get("provider", "claude").lower()
    model_name = model_config.get("model", "")
    
    if provider == "claude" or provider == "anthropic":
        return call_claude(prompt)
    elif provider == "openai" or provider == "gpt":
        return call_openai(prompt, model_name or "gpt-4")
    elif provider == "gemini" or provider == "google":
        return call_gemini(prompt)
    elif provider == "ollama":
        return call_ollama(prompt, model_name or "llama2")
    elif provider == "cohere":
        return call_cohere(prompt)
    elif provider == "huggingface" or provider == "hf":
        return call_huggingface(prompt, model_name)
    else:
        return f"[ERROR] ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì œê³µì: {provider}"

def handle_transfer(current_context, config):
    """ì´ê´€ ìš”ì²­ ì²˜ë¦¬ ë° ë¸Œë¦¿ì§€ ë°ì´í„° ìƒì„±"""
    prompt = f"""[FLOCO ì´ê´€ ì‹œìŠ¤í…œ]
- ë£¨í”„ ì´ë¦„: {config['loop_name']}
- íŒë‹¨ì: {config['judgment_owner']}
- í˜„ì¬ ì…ë ¥: "{current_context}"
- ë¸Œë¦¿ì§€ í˜•ì‹: {config.get('bridge_format', 'context_summary')}

í˜„ì¬ ì„¸ì…˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë‹¤ìŒ Claude ì„¸ì…˜ìœ¼ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•œ ìš”ì•½ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
í•µì‹¬ íŒë‹¨ ì‚¬í•­ê³¼ ì—°ì†ì„±ì´ í•„ìš”í•œ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”."""

    model_config = config.get("model", {"provider": "claude"})
    bridge_summary = call_ai_model(prompt, model_config)
    
    return f"""[FLOCO ì´ê´€ ì‹œìŠ¤í…œ í™œì„±í™”]

ğŸ”„ **ì„¸ì…˜ ë¸Œë¦¿ì§€ ìƒì„± ì™„ë£Œ**
- ë£¨í”„: {config['loop_name']}
- íŒë‹¨ì: {config['judgment_owner']}
- ëª¨ë¸: {model_config.get('provider', 'claude')}
- ì´ê´€ ìš”ì²­: "{current_context}"

ğŸ“¦ **ë‹¤ìŒ Claudeì—ê²Œ ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸**:
{bridge_summary}

ğŸ’¡ **ì‚¬ìš©ë²•**: ìœ„ ë‚´ìš©ì„ ë³µì‚¬í•´ì„œ ìƒˆë¡œìš´ Claude ì„¸ì…˜ì— ë¶™ì—¬ë„£ì–´ ì£¼ì„¸ìš”.

(ë¬´ê¸°ì–µ ë£¨í”„: íŒë‹¨ íë¦„ë§Œ ì „ë‹¬ë˜ë©°, ê³¼ê±° ëŒ€í™”ëŠ” ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤)"""

def handle_input(user_input):
    config = load_loop_config()
    
    # ëª¨ë¸ ë³€ê²½ ìš”ì²­ ìš°ì„  í™•ì¸
    is_switch, target_model = is_model_switch_triggered(user_input, config)
    if is_switch:
        if switch_model(config, target_model):
            model_info = config["model_options"][target_model]
            return f"""[FLOCO ëª¨ë¸ ë³€ê²½ ì™„ë£Œ]

- ë³€ê²½ëœ ëª¨ë¸: {model_info.get('provider', 'unknown')} ({model_info.get('model', 'default')})
- ë£¨í”„: {config['loop_name']}
- íŒë‹¨ì: {config['judgment_owner']}

(í˜„ì¬ ì„¸ì…˜ì—ì„œë§Œ ì ìš©ë©ë‹ˆë‹¤. ì˜êµ¬ ë³€ê²½ì€ loop_config.jsonì„ ìˆ˜ì •í•˜ì„¸ìš”)"""
        else:
            return "[FLOCO] ëª¨ë¸ ë³€ê²½ ì‹¤íŒ¨: ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸"
    
    # ì´ê´€ ìš”ì²­ í™•ì¸
    if is_transfer_triggered(user_input, config):
        return handle_transfer(user_input, config)
    
    if not is_triggered(user_input, config):
        return "[FLOCO] ë£¨í”„ ë¯¸ê°œë°©: íŠ¸ë¦¬ê±° ì¡°ê±´ ë¯¸ì¶©ì¡±"

    # ì‹¤ì œ AI ëª¨ë¸ íŒë‹¨ ì‹¤í–‰
    prompt = f"""[FLOCO íŒë‹¨ ë£¨í”„]
- ë£¨í”„ ì´ë¦„: {config['loop_name']}
- íŒë‹¨ì: {config['judgment_owner']}
- ì‚¬ìš©ì ì…ë ¥: "{user_input}"

ìœ„ ì…ë ¥ì— ëŒ€í•´ FLOCO êµ¬ì¡°ì— ë”°ë¥¸ íŒë‹¨ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

    model_config = config.get("model", {"provider": "claude"})
    
    # Fallback ì‹œìŠ¤í…œ ì ìš©
    fallback_order = config.get("fallback_order", [])
    ai_response = None
    used_model = None
    
    # í˜„ì¬ ëª¨ë¸ ì‹œë„
    ai_response = call_ai_model(prompt, model_config)
    if not ai_response.startswith("[ERROR]"):
        used_model = f"{model_config.get('provider', 'unknown')} ({model_config.get('model', 'default')})"
    else:
        # Fallback ëª¨ë¸ë“¤ ì‹œë„
        for fallback_model in fallback_order:
            if fallback_model in config.get("model_options", {}):
                fallback_config = config["model_options"][fallback_model]
                ai_response = call_ai_model(prompt, fallback_config)
                if not ai_response.startswith("[ERROR]"):
                    used_model = f"{fallback_config.get('provider', 'unknown')} ({fallback_config.get('model', 'default')}) [FALLBACK]"
                    break
    
    if used_model is None:
        used_model = "ERROR - All models failed"
    
    return f"""[FLOCO íŒë‹¨ ë£¨í”„ ê°œë°©ë¨]

- ë£¨í”„ ì´ë¦„: {config['loop_name']}
- íŒë‹¨ì: {config['judgment_owner']}
- ëª¨ë¸: {used_model}
- ì…ë ¥: "{user_input}"
- íŒë‹¨ ê²°ê³¼: {ai_response}

(ë¬´ê¸°ì–µ ë£¨í”„: ê³¼ê±° ë°œí™”ëŠ” ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤)"""

# CLI í…ŒìŠ¤íŠ¸ìš©
if __name__ == "__main__":
    print("[FLOCO Universal Core with Transfer System] ì§€ì› ëª¨ë¸: Claude, GPT, Gemini, Ollama, Cohere, HuggingFace")
    print("íŠ¹ë³„ ê¸°ëŠ¥: ì´ê´€ ì‹œìŠ¤í…œ, ëŸ°íƒ€ì„ ëª¨ë¸ ìŠ¤ìœ„ì¹­, ìë™ í´ë°±")
    while True:
        user_input = input("ì…ë ¥: ")
        if user_input.lower() in ["exit", "quit"]:
            break
        print(handle_input(user_input))
